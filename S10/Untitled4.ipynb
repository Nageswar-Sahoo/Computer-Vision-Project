{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkIV2g7Vwasu2sCKghfIUM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nageswar-Sahoo/Computer-Vision-Project/blob/main/S10/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IfK-I8f_hkm",
        "outputId": "5ca4fea5-7c9f-4dc4-c868-2f4b5be5deb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Computer-Vision-Project'...\n",
            "remote: Enumerating objects: 1222, done.\u001b[K\n",
            "remote: Counting objects: 100% (1222/1222), done.\u001b[K\n",
            "remote: Compressing objects: 100% (986/986), done.\u001b[K\n",
            "remote: Total 1222 (delta 772), reused 491 (delta 210), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1222/1222), 7.02 MiB | 11.58 MiB/s, done.\n",
            "Resolving deltas: 100% (772/772), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Nageswar-Sahoo/Computer-Vision-Project.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U git+https://github.com/albumentations-team/albumentations\n",
        "import albumentations \n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPTc_8dK_tEt",
        "outputId": "1d5d7c1e-8e37-42cc-f1c7-4bac9d031044"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/albumentations-team/albumentations\n",
            "  Cloning https://github.com/albumentations-team/albumentations to /tmp/pip-req-build-0702q6np\n",
            "  Running command git clone -q https://github.com/albumentations-team/albumentations /tmp/pip-req-build-0702q6np\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (1.4.1)\n",
            "Requirement already satisfied: scikit-image<0.19,>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (0.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (3.13)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (0.0.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (4.1.2.30)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.1.0) (3.10.0.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.1.0) (4.5.4.60)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.1.0) (1.0.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (1.2.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (2.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (2021.11.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.1.0) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.1.0) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Computer-Vision-Project/template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1xR9QPA_ypg",
        "outputId": "b55c02aa-0507-4e7c-d93a-e2965d91e355"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Computer-Vision-Project/template\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/seshuad/IMagenet\n",
        "! ls 'IMagenet/tiny-imagenet-200/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zd4vMKG_0yE",
        "outputId": "de92de9d-3d89-4a49-e625-3302b8ef8fd5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IMagenet'...\n",
            "remote: Enumerating objects: 120594, done.\u001b[K\n",
            "remote: Total 120594 (delta 0), reused 0 (delta 0), pack-reused 120594\u001b[K\n",
            "Receiving objects: 100% (120594/120594), 212.68 MiB | 17.29 MiB/s, done.\n",
            "Resolving deltas: 100% (1115/1115), done.\n",
            "Checking out files: 100% (120206/120206), done.\n",
            "test  train  val  wnids.txt  words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "id": "Zi46cyL5IhxJ",
        "outputId": "63c679bc-39c8-420b-cafb-9c251d2ba0f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "_4EPCWjfIk11",
        "outputId": "17b60341-367f-4f27-f8c0-a986a901ddfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Computer-Vision-Project"
      ],
      "metadata": {
        "id": "VQyBS2Q6Inqr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "P7HJ_amcIpc2",
        "outputId": "d3546904-bf56-4fc8-e93f-80730faa371d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mIMagenet\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from parse_config import ConfigParser\n",
        "from trainer import Trainer\n",
        "from utils import prepare_device\n",
        "from utils import get_splited_data\n",
        "import model.loss as module_loss\n",
        "import model.metric as module_metric\n",
        "import torch.optim as optim\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import model.customresnet as module_resnet\n",
        "import utils\n",
        "SEED = 123\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(SEED)\n",
        "import torch\n",
        "import data_loader.data_loaders as data_loaders\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "XkP2YI4cAAM2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    logger = logging.getLogger(\"trian\")\n",
        "    # Read the config.json\n",
        "    config = ConfigParser.from_args()\n",
        "    train_data_u, train_labels_u, test_data, test_labels = get_splited_data()\n",
        "    train_labels_u=train_labels_u.astype(float)\n",
        "    test_labels=test_labels.astype(float)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    data_loader = data_loaders.get_train_data_loader(train_data_u, train_labels_u, 64)\n",
        "\n",
        "    valid_data_loader = data_loaders.get_test_data_loader(test_data, test_labels, 64)\n",
        "\n",
        "    # build model architecture, then print to console\n",
        "    model = module_resnet.CustomResnet()\n",
        "    #Load Resnet18\n",
        "    model = models.resnet18(True)\n",
        "    #Finetune Final few layers to adjust for tiny imagenet input\n",
        "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    model.fc.out_features = 200\n",
        "\n",
        "\n",
        "\n",
        "    logger.info(model)\n",
        "\n",
        "    # prepare for (multi-device) GPU training\n",
        "    n_gpu = 1\n",
        "    device, device_ids = prepare_device(n_gpu)\n",
        "    model = model.to(device)\n",
        "    if len(device_ids) > 1:\n",
        "        model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "\n",
        "    # get function handles of loss and metrics\n",
        "    criterion = module_loss.crossentropyloss\n",
        "    metrics = [module_metric.accuracy]\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=.01, weight_decay=1e-4)\n",
        "    # lr_scheduler = StepLR(optimizer, step_size=15, gamma=0.001)\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, cycle_momentum=True, anneal_strategy='linear', verbose=False,\n",
        "                                              three_phase=True, max_lr=.01, pct_start=5 / 24, epochs=24,\n",
        "                                              steps_per_epoch=len(data_loader))\n",
        "\n",
        "    trainer = Trainer(model, criterion, metrics, optimizer,\n",
        "                      config=config,\n",
        "                      device=device,\n",
        "                      data_loader=data_loader,\n",
        "                      valid_data_loader=valid_data_loader,\n",
        "                      lr_scheduler=scheduler)\n",
        "\n"
      ],
      "metadata": {
        "id": "nFTaGYfqAC8Y",
        "outputId": "cd269665-fea3-47f4-817a-7ef1097160b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting loading data\n",
            "finished loading data, in 15.683183908462524 seconds\n",
            "test data shape:  (30000, 64, 64, 3)\n",
            "test_labels.shape:  (30000, 200)\n",
            "train data shape:  (80000, 64, 64, 3)\n",
            "train label shape:  (80000, 200)\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (fc): Linear(in_features=512, out_features=200, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "SePSxk-6AE3D",
        "outputId": "934ed0c5-0b4a-477e-ab84-11ef665e4831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 last_lr_used  0.000402  [0/1250 (0%)] Loss: 15.254631\n",
            "Train Epoch: 1 last_lr_used  0.000414  [8/1250 (1%)] Loss: 9.724217\n",
            "Train Epoch: 1 last_lr_used  0.000426  [16/1250 (1%)] Loss: 6.633124\n",
            "Train Epoch: 1 last_lr_used  0.000438  [24/1250 (2%)] Loss: 6.287477\n",
            "Train Epoch: 1 last_lr_used  0.000451  [32/1250 (3%)] Loss: 5.825970\n",
            "Train Epoch: 1 last_lr_used  0.000463  [40/1250 (3%)] Loss: 5.600729\n",
            "Train Epoch: 1 last_lr_used  0.000475  [48/1250 (4%)] Loss: 5.567503\n",
            "Train Epoch: 1 last_lr_used  0.000488  [56/1250 (4%)] Loss: 5.501855\n",
            "Train Epoch: 1 last_lr_used  0.000500  [64/1250 (5%)] Loss: 5.316102\n",
            "Train Epoch: 1 last_lr_used  0.000512  [72/1250 (6%)] Loss: 5.402303\n",
            "Train Epoch: 1 last_lr_used  0.000524  [80/1250 (6%)] Loss: 5.539340\n",
            "Train Epoch: 1 last_lr_used  0.000537  [88/1250 (7%)] Loss: 5.175946\n",
            "Train Epoch: 1 last_lr_used  0.000549  [96/1250 (8%)] Loss: 5.266450\n",
            "Train Epoch: 1 last_lr_used  0.000561  [104/1250 (8%)] Loss: 5.298096\n",
            "Train Epoch: 1 last_lr_used  0.000574  [112/1250 (9%)] Loss: 5.157065\n",
            "Train Epoch: 1 last_lr_used  0.000586  [120/1250 (10%)] Loss: 5.209152\n",
            "Train Epoch: 1 last_lr_used  0.000598  [128/1250 (10%)] Loss: 5.259694\n",
            "Train Epoch: 1 last_lr_used  0.000610  [136/1250 (11%)] Loss: 5.123481\n",
            "Train Epoch: 1 last_lr_used  0.000623  [144/1250 (12%)] Loss: 5.031784\n",
            "Train Epoch: 1 last_lr_used  0.000635  [152/1250 (12%)] Loss: 4.922102\n",
            "Train Epoch: 1 last_lr_used  0.000647  [160/1250 (13%)] Loss: 4.940948\n",
            "Train Epoch: 1 last_lr_used  0.000660  [168/1250 (13%)] Loss: 5.012390\n",
            "Train Epoch: 1 last_lr_used  0.000672  [176/1250 (14%)] Loss: 5.099337\n",
            "Train Epoch: 1 last_lr_used  0.000684  [184/1250 (15%)] Loss: 5.148364\n",
            "Train Epoch: 1 last_lr_used  0.000696  [192/1250 (15%)] Loss: 4.919691\n",
            "Train Epoch: 1 last_lr_used  0.000709  [200/1250 (16%)] Loss: 4.315781\n",
            "Train Epoch: 1 last_lr_used  0.000721  [208/1250 (17%)] Loss: 4.909160\n",
            "Train Epoch: 1 last_lr_used  0.000733  [216/1250 (17%)] Loss: 4.680020\n",
            "Train Epoch: 1 last_lr_used  0.000746  [224/1250 (18%)] Loss: 4.468668\n",
            "Train Epoch: 1 last_lr_used  0.000758  [232/1250 (19%)] Loss: 4.803223\n",
            "Train Epoch: 1 last_lr_used  0.000770  [240/1250 (19%)] Loss: 4.262184\n",
            "Train Epoch: 1 last_lr_used  0.000783  [248/1250 (20%)] Loss: 4.284447\n",
            "Train Epoch: 1 last_lr_used  0.000795  [256/1250 (20%)] Loss: 4.390624\n",
            "Train Epoch: 1 last_lr_used  0.000807  [264/1250 (21%)] Loss: 4.375547\n",
            "Train Epoch: 1 last_lr_used  0.000819  [272/1250 (22%)] Loss: 4.359176\n",
            "Train Epoch: 1 last_lr_used  0.000832  [280/1250 (22%)] Loss: 4.456079\n",
            "Train Epoch: 1 last_lr_used  0.000844  [288/1250 (23%)] Loss: 4.278139\n",
            "Train Epoch: 1 last_lr_used  0.000856  [296/1250 (24%)] Loss: 4.171981\n",
            "Train Epoch: 1 last_lr_used  0.000869  [304/1250 (24%)] Loss: 3.917676\n",
            "Train Epoch: 1 last_lr_used  0.000881  [312/1250 (25%)] Loss: 4.312005\n",
            "Train Epoch: 1 last_lr_used  0.000893  [320/1250 (26%)] Loss: 3.789247\n",
            "Train Epoch: 1 last_lr_used  0.000905  [328/1250 (26%)] Loss: 4.007707\n",
            "Train Epoch: 1 last_lr_used  0.000918  [336/1250 (27%)] Loss: 4.072332\n",
            "Train Epoch: 1 last_lr_used  0.000930  [344/1250 (28%)] Loss: 3.938106\n",
            "Train Epoch: 1 last_lr_used  0.000942  [352/1250 (28%)] Loss: 4.038226\n",
            "Train Epoch: 1 last_lr_used  0.000955  [360/1250 (29%)] Loss: 3.871560\n",
            "Train Epoch: 1 last_lr_used  0.000967  [368/1250 (29%)] Loss: 3.827309\n",
            "Train Epoch: 1 last_lr_used  0.000979  [376/1250 (30%)] Loss: 4.448049\n",
            "Train Epoch: 1 last_lr_used  0.000991  [384/1250 (31%)] Loss: 3.857756\n",
            "Train Epoch: 1 last_lr_used  0.001004  [392/1250 (31%)] Loss: 3.629043\n",
            "Train Epoch: 1 last_lr_used  0.001016  [400/1250 (32%)] Loss: 3.963039\n",
            "Train Epoch: 1 last_lr_used  0.001028  [408/1250 (33%)] Loss: 3.891646\n",
            "Train Epoch: 1 last_lr_used  0.001041  [416/1250 (33%)] Loss: 3.767997\n",
            "Train Epoch: 1 last_lr_used  0.001053  [424/1250 (34%)] Loss: 3.825854\n",
            "Train Epoch: 1 last_lr_used  0.001065  [432/1250 (35%)] Loss: 3.979200\n",
            "Train Epoch: 1 last_lr_used  0.001077  [440/1250 (35%)] Loss: 3.751706\n",
            "Train Epoch: 1 last_lr_used  0.001090  [448/1250 (36%)] Loss: 3.535192\n",
            "Train Epoch: 1 last_lr_used  0.001102  [456/1250 (36%)] Loss: 3.869735\n",
            "Train Epoch: 1 last_lr_used  0.001114  [464/1250 (37%)] Loss: 3.696221\n",
            "Train Epoch: 1 last_lr_used  0.001127  [472/1250 (38%)] Loss: 3.877799\n",
            "Train Epoch: 1 last_lr_used  0.001139  [480/1250 (38%)] Loss: 3.583655\n",
            "Train Epoch: 1 last_lr_used  0.001151  [488/1250 (39%)] Loss: 3.676367\n",
            "Train Epoch: 1 last_lr_used  0.001164  [496/1250 (40%)] Loss: 3.636446\n",
            "Train Epoch: 1 last_lr_used  0.001176  [504/1250 (40%)] Loss: 3.640191\n",
            "Train Epoch: 1 last_lr_used  0.001188  [512/1250 (41%)] Loss: 2.968233\n",
            "Train Epoch: 1 last_lr_used  0.001200  [520/1250 (42%)] Loss: 3.304862\n",
            "Train Epoch: 1 last_lr_used  0.001213  [528/1250 (42%)] Loss: 4.044750\n",
            "Train Epoch: 1 last_lr_used  0.001225  [536/1250 (43%)] Loss: 4.298878\n",
            "Train Epoch: 1 last_lr_used  0.001237  [544/1250 (44%)] Loss: 3.927137\n",
            "Train Epoch: 1 last_lr_used  0.001250  [552/1250 (44%)] Loss: 3.858428\n",
            "Train Epoch: 1 last_lr_used  0.001262  [560/1250 (45%)] Loss: 3.659713\n",
            "Train Epoch: 1 last_lr_used  0.001274  [568/1250 (45%)] Loss: 3.581729\n",
            "Train Epoch: 1 last_lr_used  0.001286  [576/1250 (46%)] Loss: 3.513175\n",
            "Train Epoch: 1 last_lr_used  0.001299  [584/1250 (47%)] Loss: 3.878544\n",
            "Train Epoch: 1 last_lr_used  0.001311  [592/1250 (47%)] Loss: 4.069404\n",
            "Train Epoch: 1 last_lr_used  0.001323  [600/1250 (48%)] Loss: 3.815376\n",
            "Train Epoch: 1 last_lr_used  0.001336  [608/1250 (49%)] Loss: 3.454167\n",
            "Train Epoch: 1 last_lr_used  0.001348  [616/1250 (49%)] Loss: 3.761541\n",
            "Train Epoch: 1 last_lr_used  0.001360  [624/1250 (50%)] Loss: 3.497631\n",
            "Train Epoch: 1 last_lr_used  0.001372  [632/1250 (51%)] Loss: 3.737667\n",
            "Train Epoch: 1 last_lr_used  0.001385  [640/1250 (51%)] Loss: 3.407194\n",
            "Train Epoch: 1 last_lr_used  0.001397  [648/1250 (52%)] Loss: 3.788117\n",
            "Train Epoch: 1 last_lr_used  0.001409  [656/1250 (52%)] Loss: 3.533340\n",
            "Train Epoch: 1 last_lr_used  0.001422  [664/1250 (53%)] Loss: 3.687558\n",
            "Train Epoch: 1 last_lr_used  0.001434  [672/1250 (54%)] Loss: 3.780117\n",
            "Train Epoch: 1 last_lr_used  0.001446  [680/1250 (54%)] Loss: 3.721111\n",
            "Train Epoch: 1 last_lr_used  0.001458  [688/1250 (55%)] Loss: 3.540361\n",
            "Train Epoch: 1 last_lr_used  0.001471  [696/1250 (56%)] Loss: 3.530292\n",
            "Train Epoch: 1 last_lr_used  0.001483  [704/1250 (56%)] Loss: 3.660273\n",
            "Train Epoch: 1 last_lr_used  0.001495  [712/1250 (57%)] Loss: 3.975588\n",
            "Train Epoch: 1 last_lr_used  0.001508  [720/1250 (58%)] Loss: 3.550091\n",
            "Train Epoch: 1 last_lr_used  0.001520  [728/1250 (58%)] Loss: 3.363702\n",
            "Train Epoch: 1 last_lr_used  0.001532  [736/1250 (59%)] Loss: 3.506877\n",
            "Train Epoch: 1 last_lr_used  0.001545  [744/1250 (60%)] Loss: 3.596758\n",
            "Train Epoch: 1 last_lr_used  0.001557  [752/1250 (60%)] Loss: 3.810376\n",
            "Train Epoch: 1 last_lr_used  0.001569  [760/1250 (61%)] Loss: 3.828776\n",
            "Train Epoch: 1 last_lr_used  0.001581  [768/1250 (61%)] Loss: 3.970759\n",
            "Train Epoch: 1 last_lr_used  0.001594  [776/1250 (62%)] Loss: 3.849221\n",
            "Train Epoch: 1 last_lr_used  0.001606  [784/1250 (63%)] Loss: 3.939610\n",
            "Train Epoch: 1 last_lr_used  0.001618  [792/1250 (63%)] Loss: 3.751855\n",
            "Train Epoch: 1 last_lr_used  0.001631  [800/1250 (64%)] Loss: 3.524027\n",
            "Train Epoch: 1 last_lr_used  0.001643  [808/1250 (65%)] Loss: 3.356779\n",
            "Train Epoch: 1 last_lr_used  0.001655  [816/1250 (65%)] Loss: 3.280684\n",
            "Train Epoch: 1 last_lr_used  0.001667  [824/1250 (66%)] Loss: 3.625760\n",
            "Train Epoch: 1 last_lr_used  0.001680  [832/1250 (67%)] Loss: 3.660141\n",
            "Train Epoch: 1 last_lr_used  0.001692  [840/1250 (67%)] Loss: 3.542537\n",
            "Train Epoch: 1 last_lr_used  0.001704  [848/1250 (68%)] Loss: 3.499792\n",
            "Train Epoch: 1 last_lr_used  0.001717  [856/1250 (68%)] Loss: 3.592192\n",
            "Train Epoch: 1 last_lr_used  0.001729  [864/1250 (69%)] Loss: 3.255863\n",
            "Train Epoch: 1 last_lr_used  0.001741  [872/1250 (70%)] Loss: 3.229956\n",
            "Train Epoch: 1 last_lr_used  0.001753  [880/1250 (70%)] Loss: 3.815185\n",
            "Train Epoch: 1 last_lr_used  0.001766  [888/1250 (71%)] Loss: 3.311444\n",
            "Train Epoch: 1 last_lr_used  0.001778  [896/1250 (72%)] Loss: 3.623878\n",
            "Train Epoch: 1 last_lr_used  0.001790  [904/1250 (72%)] Loss: 4.081101\n",
            "Train Epoch: 1 last_lr_used  0.001803  [912/1250 (73%)] Loss: 3.870508\n",
            "Train Epoch: 1 last_lr_used  0.001815  [920/1250 (74%)] Loss: 3.682268\n",
            "Train Epoch: 1 last_lr_used  0.001827  [928/1250 (74%)] Loss: 3.663171\n",
            "Train Epoch: 1 last_lr_used  0.001839  [936/1250 (75%)] Loss: 3.842176\n",
            "Train Epoch: 1 last_lr_used  0.001852  [944/1250 (76%)] Loss: 3.669478\n",
            "Train Epoch: 1 last_lr_used  0.001864  [952/1250 (76%)] Loss: 3.338031\n",
            "Train Epoch: 1 last_lr_used  0.001876  [960/1250 (77%)] Loss: 3.815816\n",
            "Train Epoch: 1 last_lr_used  0.001889  [968/1250 (77%)] Loss: 3.576452\n",
            "Train Epoch: 1 last_lr_used  0.001901  [976/1250 (78%)] Loss: 3.679426\n",
            "Train Epoch: 1 last_lr_used  0.001913  [984/1250 (79%)] Loss: 3.703114\n",
            "Train Epoch: 1 last_lr_used  0.001925  [992/1250 (79%)] Loss: 3.429160\n",
            "Train Epoch: 1 last_lr_used  0.001938  [1000/1250 (80%)] Loss: 3.250765\n",
            "Train Epoch: 1 last_lr_used  0.001950  [1008/1250 (81%)] Loss: 3.588489\n",
            "Train Epoch: 1 last_lr_used  0.001962  [1016/1250 (81%)] Loss: 3.890028\n",
            "Train Epoch: 1 last_lr_used  0.001975  [1024/1250 (82%)] Loss: 3.636345\n",
            "Train Epoch: 1 last_lr_used  0.001987  [1032/1250 (83%)] Loss: 3.409773\n",
            "Train Epoch: 1 last_lr_used  0.001999  [1040/1250 (83%)] Loss: 3.522929\n",
            "Train Epoch: 1 last_lr_used  0.002012  [1048/1250 (84%)] Loss: 3.622776\n",
            "Train Epoch: 1 last_lr_used  0.002024  [1056/1250 (84%)] Loss: 4.184646\n",
            "Train Epoch: 1 last_lr_used  0.002036  [1064/1250 (85%)] Loss: 3.623168\n",
            "Train Epoch: 1 last_lr_used  0.002048  [1072/1250 (86%)] Loss: 4.075846\n",
            "Train Epoch: 1 last_lr_used  0.002061  [1080/1250 (86%)] Loss: 3.773385\n",
            "Train Epoch: 1 last_lr_used  0.002073  [1088/1250 (87%)] Loss: 3.362180\n",
            "Train Epoch: 1 last_lr_used  0.002085  [1096/1250 (88%)] Loss: 3.393600\n",
            "Train Epoch: 1 last_lr_used  0.002098  [1104/1250 (88%)] Loss: 3.711448\n",
            "Train Epoch: 1 last_lr_used  0.002110  [1112/1250 (89%)] Loss: 3.380654\n",
            "Train Epoch: 1 last_lr_used  0.002122  [1120/1250 (90%)] Loss: 3.270726\n",
            "Train Epoch: 1 last_lr_used  0.002134  [1128/1250 (90%)] Loss: 3.528203\n",
            "Train Epoch: 1 last_lr_used  0.002147  [1136/1250 (91%)] Loss: 3.524648\n",
            "Train Epoch: 1 last_lr_used  0.002159  [1144/1250 (92%)] Loss: 3.727500\n",
            "Train Epoch: 1 last_lr_used  0.002171  [1152/1250 (92%)] Loss: 3.522099\n",
            "Train Epoch: 1 last_lr_used  0.002184  [1160/1250 (93%)] Loss: 4.420233\n",
            "Train Epoch: 1 last_lr_used  0.002196  [1168/1250 (93%)] Loss: 4.146620\n",
            "Train Epoch: 1 last_lr_used  0.002208  [1176/1250 (94%)] Loss: 3.742412\n",
            "Train Epoch: 1 last_lr_used  0.002220  [1184/1250 (95%)] Loss: 3.636450\n",
            "Train Epoch: 1 last_lr_used  0.002233  [1192/1250 (95%)] Loss: 3.542921\n",
            "Train Epoch: 1 last_lr_used  0.002245  [1200/1250 (96%)] Loss: 3.523323\n",
            "Train Epoch: 1 last_lr_used  0.002257  [1208/1250 (97%)] Loss: 3.649901\n",
            "Train Epoch: 1 last_lr_used  0.002270  [1216/1250 (97%)] Loss: 3.684527\n",
            "Train Epoch: 1 last_lr_used  0.002282  [1224/1250 (98%)] Loss: 3.331346\n",
            "Train Epoch: 1 last_lr_used  0.002294  [1232/1250 (99%)] Loss: 3.796543\n",
            "Train Epoch: 1 last_lr_used  0.002306  [1240/1250 (99%)] Loss: 3.833354\n",
            "Train Epoch: 1 last_lr_used  0.002319  [1248/1250 (100%)] Loss: 3.740295\n",
            "<utils.util.MetricTracker object at 0x7ff5e6f9f910>\n",
            "    epoch          : 1\n",
            "    loss           : 4.099059843444825\n",
            "    accuracy       : 15.23875\n",
            "    val_loss       : 3.852826894727597\n",
            "    val_accuracy   : 17.434035181236673\n",
            "Saving checkpoint: saved/models/TinyImageNet/1209_134233/checkpoint-epoch1.pth ...\n",
            "Saving current best: model_best.pth ...\n",
            "Train Epoch: 2 last_lr_used  0.002322  [0/1250 (0%)] Loss: 3.344025\n",
            "Train Epoch: 2 last_lr_used  0.002334  [8/1250 (1%)] Loss: 3.454443\n",
            "Train Epoch: 2 last_lr_used  0.002346  [16/1250 (1%)] Loss: 4.020256\n",
            "Train Epoch: 2 last_lr_used  0.002359  [24/1250 (2%)] Loss: 4.125555\n",
            "Train Epoch: 2 last_lr_used  0.002371  [32/1250 (3%)] Loss: 3.961865\n",
            "Train Epoch: 2 last_lr_used  0.002383  [40/1250 (3%)] Loss: 3.776708\n",
            "Train Epoch: 2 last_lr_used  0.002396  [48/1250 (4%)] Loss: 3.762969\n",
            "Train Epoch: 2 last_lr_used  0.002408  [56/1250 (4%)] Loss: 3.666880\n",
            "Train Epoch: 2 last_lr_used  0.002420  [64/1250 (5%)] Loss: 4.041777\n",
            "Train Epoch: 2 last_lr_used  0.002432  [72/1250 (6%)] Loss: 3.692468\n",
            "Train Epoch: 2 last_lr_used  0.002445  [80/1250 (6%)] Loss: 3.634257\n",
            "Train Epoch: 2 last_lr_used  0.002457  [88/1250 (7%)] Loss: 3.173725\n",
            "Train Epoch: 2 last_lr_used  0.002469  [96/1250 (8%)] Loss: 3.464008\n",
            "Train Epoch: 2 last_lr_used  0.002482  [104/1250 (8%)] Loss: 3.899969\n",
            "Train Epoch: 2 last_lr_used  0.002494  [112/1250 (9%)] Loss: 3.775578\n",
            "Train Epoch: 2 last_lr_used  0.002506  [120/1250 (10%)] Loss: 3.889663\n",
            "Train Epoch: 2 last_lr_used  0.002518  [128/1250 (10%)] Loss: 3.656724\n",
            "Train Epoch: 2 last_lr_used  0.002531  [136/1250 (11%)] Loss: 3.431937\n",
            "Train Epoch: 2 last_lr_used  0.002543  [144/1250 (12%)] Loss: 3.928663\n",
            "Train Epoch: 2 last_lr_used  0.002555  [152/1250 (12%)] Loss: 3.595214\n",
            "Train Epoch: 2 last_lr_used  0.002568  [160/1250 (13%)] Loss: 4.426229\n",
            "Train Epoch: 2 last_lr_used  0.002580  [168/1250 (13%)] Loss: 3.503003\n",
            "Train Epoch: 2 last_lr_used  0.002592  [176/1250 (14%)] Loss: 3.744760\n",
            "Train Epoch: 2 last_lr_used  0.002605  [184/1250 (15%)] Loss: 4.044010\n",
            "Train Epoch: 2 last_lr_used  0.002617  [192/1250 (15%)] Loss: 3.868779\n",
            "Train Epoch: 2 last_lr_used  0.002629  [200/1250 (16%)] Loss: 3.624044\n",
            "Train Epoch: 2 last_lr_used  0.002641  [208/1250 (17%)] Loss: 3.702259\n",
            "Train Epoch: 2 last_lr_used  0.002654  [216/1250 (17%)] Loss: 3.314161\n",
            "Train Epoch: 2 last_lr_used  0.002666  [224/1250 (18%)] Loss: 3.416903\n",
            "Train Epoch: 2 last_lr_used  0.002678  [232/1250 (19%)] Loss: 3.593563\n",
            "Train Epoch: 2 last_lr_used  0.002691  [240/1250 (19%)] Loss: 3.875825\n",
            "Train Epoch: 2 last_lr_used  0.002703  [248/1250 (20%)] Loss: 3.790463\n",
            "Train Epoch: 2 last_lr_used  0.002715  [256/1250 (20%)] Loss: 3.968727\n",
            "Train Epoch: 2 last_lr_used  0.002727  [264/1250 (21%)] Loss: 3.726248\n",
            "Train Epoch: 2 last_lr_used  0.002740  [272/1250 (22%)] Loss: 3.502714\n",
            "Train Epoch: 2 last_lr_used  0.002752  [280/1250 (22%)] Loss: 3.775687\n",
            "Train Epoch: 2 last_lr_used  0.002764  [288/1250 (23%)] Loss: 3.606369\n",
            "Train Epoch: 2 last_lr_used  0.002777  [296/1250 (24%)] Loss: 3.650620\n",
            "Train Epoch: 2 last_lr_used  0.002789  [304/1250 (24%)] Loss: 3.650651\n",
            "Train Epoch: 2 last_lr_used  0.002801  [312/1250 (25%)] Loss: 3.275399\n",
            "Train Epoch: 2 last_lr_used  0.002813  [320/1250 (26%)] Loss: 4.002762\n",
            "Train Epoch: 2 last_lr_used  0.002826  [328/1250 (26%)] Loss: 3.866288\n",
            "Train Epoch: 2 last_lr_used  0.002838  [336/1250 (27%)] Loss: 3.493937\n",
            "Train Epoch: 2 last_lr_used  0.002850  [344/1250 (28%)] Loss: 3.572425\n",
            "Train Epoch: 2 last_lr_used  0.002863  [352/1250 (28%)] Loss: 3.454750\n",
            "Train Epoch: 2 last_lr_used  0.002875  [360/1250 (29%)] Loss: 3.728877\n",
            "Train Epoch: 2 last_lr_used  0.002887  [368/1250 (29%)] Loss: 3.936296\n",
            "Train Epoch: 2 last_lr_used  0.002899  [376/1250 (30%)] Loss: 3.166186\n",
            "Train Epoch: 2 last_lr_used  0.002912  [384/1250 (31%)] Loss: 3.422954\n",
            "Train Epoch: 2 last_lr_used  0.002924  [392/1250 (31%)] Loss: 3.994381\n",
            "Train Epoch: 2 last_lr_used  0.002936  [400/1250 (32%)] Loss: 3.652256\n",
            "Train Epoch: 2 last_lr_used  0.002949  [408/1250 (33%)] Loss: 3.394543\n",
            "Train Epoch: 2 last_lr_used  0.002961  [416/1250 (33%)] Loss: 3.967180\n",
            "Train Epoch: 2 last_lr_used  0.002973  [424/1250 (34%)] Loss: 3.141608\n",
            "Train Epoch: 2 last_lr_used  0.002986  [432/1250 (35%)] Loss: 3.707330\n",
            "Train Epoch: 2 last_lr_used  0.002998  [440/1250 (35%)] Loss: 3.562908\n",
            "Train Epoch: 2 last_lr_used  0.003010  [448/1250 (36%)] Loss: 3.535850\n",
            "Train Epoch: 2 last_lr_used  0.003022  [456/1250 (36%)] Loss: 3.790048\n",
            "Train Epoch: 2 last_lr_used  0.003035  [464/1250 (37%)] Loss: 3.529212\n",
            "Train Epoch: 2 last_lr_used  0.003047  [472/1250 (38%)] Loss: 3.719532\n",
            "Train Epoch: 2 last_lr_used  0.003059  [480/1250 (38%)] Loss: 3.414634\n",
            "Train Epoch: 2 last_lr_used  0.003072  [488/1250 (39%)] Loss: 3.510263\n",
            "Train Epoch: 2 last_lr_used  0.003084  [496/1250 (40%)] Loss: 3.863035\n",
            "Train Epoch: 2 last_lr_used  0.003096  [504/1250 (40%)] Loss: 3.767318\n",
            "Train Epoch: 2 last_lr_used  0.003108  [512/1250 (41%)] Loss: 3.649250\n",
            "Train Epoch: 2 last_lr_used  0.003121  [520/1250 (42%)] Loss: 3.380440\n",
            "Train Epoch: 2 last_lr_used  0.003133  [528/1250 (42%)] Loss: 3.824450\n",
            "Train Epoch: 2 last_lr_used  0.003145  [536/1250 (43%)] Loss: 3.515680\n",
            "Train Epoch: 2 last_lr_used  0.003158  [544/1250 (44%)] Loss: 3.676493\n",
            "Train Epoch: 2 last_lr_used  0.003170  [552/1250 (44%)] Loss: 3.517949\n",
            "Train Epoch: 2 last_lr_used  0.003182  [560/1250 (45%)] Loss: 3.311791\n",
            "Train Epoch: 2 last_lr_used  0.003194  [568/1250 (45%)] Loss: 3.963084\n",
            "Train Epoch: 2 last_lr_used  0.003207  [576/1250 (46%)] Loss: 3.788916\n",
            "Train Epoch: 2 last_lr_used  0.003219  [584/1250 (47%)] Loss: 3.596640\n",
            "Train Epoch: 2 last_lr_used  0.003231  [592/1250 (47%)] Loss: 3.310555\n",
            "Train Epoch: 2 last_lr_used  0.003244  [600/1250 (48%)] Loss: 3.878131\n",
            "Train Epoch: 2 last_lr_used  0.003256  [608/1250 (49%)] Loss: 4.075179\n",
            "Train Epoch: 2 last_lr_used  0.003268  [616/1250 (49%)] Loss: 3.452270\n",
            "Train Epoch: 2 last_lr_used  0.003280  [624/1250 (50%)] Loss: 3.239445\n",
            "Train Epoch: 2 last_lr_used  0.003293  [632/1250 (51%)] Loss: 3.818340\n",
            "Train Epoch: 2 last_lr_used  0.003305  [640/1250 (51%)] Loss: 3.521250\n",
            "Train Epoch: 2 last_lr_used  0.003317  [648/1250 (52%)] Loss: 3.836385\n",
            "Train Epoch: 2 last_lr_used  0.003330  [656/1250 (52%)] Loss: 3.525800\n",
            "Train Epoch: 2 last_lr_used  0.003342  [664/1250 (53%)] Loss: 3.635824\n",
            "Train Epoch: 2 last_lr_used  0.003354  [672/1250 (54%)] Loss: 3.717484\n",
            "Train Epoch: 2 last_lr_used  0.003366  [680/1250 (54%)] Loss: 3.332238\n",
            "Train Epoch: 2 last_lr_used  0.003379  [688/1250 (55%)] Loss: 3.268017\n",
            "Train Epoch: 2 last_lr_used  0.003391  [696/1250 (56%)] Loss: 3.666537\n",
            "Train Epoch: 2 last_lr_used  0.003403  [704/1250 (56%)] Loss: 3.493468\n",
            "Train Epoch: 2 last_lr_used  0.003416  [712/1250 (57%)] Loss: 3.716095\n",
            "Train Epoch: 2 last_lr_used  0.003428  [720/1250 (58%)] Loss: 3.280971\n",
            "Train Epoch: 2 last_lr_used  0.003440  [728/1250 (58%)] Loss: 3.511872\n",
            "Train Epoch: 2 last_lr_used  0.003453  [736/1250 (59%)] Loss: 3.374063\n",
            "Train Epoch: 2 last_lr_used  0.003465  [744/1250 (60%)] Loss: 3.089995\n",
            "Train Epoch: 2 last_lr_used  0.003477  [752/1250 (60%)] Loss: 3.793872\n",
            "Train Epoch: 2 last_lr_used  0.003489  [760/1250 (61%)] Loss: 4.051508\n",
            "Train Epoch: 2 last_lr_used  0.003502  [768/1250 (61%)] Loss: 3.814073\n",
            "Train Epoch: 2 last_lr_used  0.003514  [776/1250 (62%)] Loss: 3.467710\n",
            "Train Epoch: 2 last_lr_used  0.003526  [784/1250 (63%)] Loss: 3.955940\n",
            "Train Epoch: 2 last_lr_used  0.003539  [792/1250 (63%)] Loss: 3.890178\n",
            "Train Epoch: 2 last_lr_used  0.003551  [800/1250 (64%)] Loss: 3.286681\n",
            "Train Epoch: 2 last_lr_used  0.003563  [808/1250 (65%)] Loss: 3.729983\n",
            "Train Epoch: 2 last_lr_used  0.003575  [816/1250 (65%)] Loss: 3.483388\n",
            "Train Epoch: 2 last_lr_used  0.003588  [824/1250 (66%)] Loss: 3.559058\n",
            "Train Epoch: 2 last_lr_used  0.003600  [832/1250 (67%)] Loss: 3.561872\n",
            "Train Epoch: 2 last_lr_used  0.003612  [840/1250 (67%)] Loss: 3.736762\n",
            "Train Epoch: 2 last_lr_used  0.003625  [848/1250 (68%)] Loss: 3.430535\n",
            "Train Epoch: 2 last_lr_used  0.003637  [856/1250 (68%)] Loss: 3.215647\n",
            "Train Epoch: 2 last_lr_used  0.003649  [864/1250 (69%)] Loss: 3.377075\n",
            "Train Epoch: 2 last_lr_used  0.003661  [872/1250 (70%)] Loss: 3.173956\n",
            "Train Epoch: 2 last_lr_used  0.003674  [880/1250 (70%)] Loss: 3.510162\n",
            "Train Epoch: 2 last_lr_used  0.003686  [888/1250 (71%)] Loss: 3.547605\n",
            "Train Epoch: 2 last_lr_used  0.003698  [896/1250 (72%)] Loss: 3.815480\n",
            "Train Epoch: 2 last_lr_used  0.003711  [904/1250 (72%)] Loss: 3.407398\n",
            "Train Epoch: 2 last_lr_used  0.003723  [912/1250 (73%)] Loss: 3.322857\n",
            "Train Epoch: 2 last_lr_used  0.003735  [920/1250 (74%)] Loss: 3.340976\n",
            "Train Epoch: 2 last_lr_used  0.003747  [928/1250 (74%)] Loss: 3.206634\n",
            "Train Epoch: 2 last_lr_used  0.003760  [936/1250 (75%)] Loss: 3.630746\n",
            "Train Epoch: 2 last_lr_used  0.003772  [944/1250 (76%)] Loss: 3.545030\n",
            "Train Epoch: 2 last_lr_used  0.003784  [952/1250 (76%)] Loss: 3.411441\n",
            "Train Epoch: 2 last_lr_used  0.003797  [960/1250 (77%)] Loss: 3.315582\n",
            "Train Epoch: 2 last_lr_used  0.003809  [968/1250 (77%)] Loss: 3.250022\n",
            "Train Epoch: 2 last_lr_used  0.003821  [976/1250 (78%)] Loss: 3.491159\n",
            "Train Epoch: 2 last_lr_used  0.003834  [984/1250 (79%)] Loss: 3.829342\n",
            "Train Epoch: 2 last_lr_used  0.003846  [992/1250 (79%)] Loss: 3.288193\n",
            "Train Epoch: 2 last_lr_used  0.003858  [1000/1250 (80%)] Loss: 3.559625\n",
            "Train Epoch: 2 last_lr_used  0.003870  [1008/1250 (81%)] Loss: 3.690821\n",
            "Train Epoch: 2 last_lr_used  0.003883  [1016/1250 (81%)] Loss: 3.212888\n",
            "Train Epoch: 2 last_lr_used  0.003895  [1024/1250 (82%)] Loss: 3.641209\n",
            "Train Epoch: 2 last_lr_used  0.003907  [1032/1250 (83%)] Loss: 3.573529\n",
            "Train Epoch: 2 last_lr_used  0.003920  [1040/1250 (83%)] Loss: 2.794577\n",
            "Train Epoch: 2 last_lr_used  0.003932  [1048/1250 (84%)] Loss: 3.810041\n",
            "Train Epoch: 2 last_lr_used  0.003944  [1056/1250 (84%)] Loss: 3.843039\n",
            "Train Epoch: 2 last_lr_used  0.003956  [1064/1250 (85%)] Loss: 3.620963\n",
            "Train Epoch: 2 last_lr_used  0.003969  [1072/1250 (86%)] Loss: 3.264661\n",
            "Train Epoch: 2 last_lr_used  0.003981  [1080/1250 (86%)] Loss: 3.797200\n",
            "Train Epoch: 2 last_lr_used  0.003993  [1088/1250 (87%)] Loss: 3.746545\n",
            "Train Epoch: 2 last_lr_used  0.004006  [1096/1250 (88%)] Loss: 3.844589\n",
            "Train Epoch: 2 last_lr_used  0.004018  [1104/1250 (88%)] Loss: 3.672207\n",
            "Train Epoch: 2 last_lr_used  0.004030  [1112/1250 (89%)] Loss: 3.456986\n",
            "Train Epoch: 2 last_lr_used  0.004042  [1120/1250 (90%)] Loss: 3.263252\n",
            "Train Epoch: 2 last_lr_used  0.004055  [1128/1250 (90%)] Loss: 3.580008\n",
            "Train Epoch: 2 last_lr_used  0.004067  [1136/1250 (91%)] Loss: 3.127179\n",
            "Train Epoch: 2 last_lr_used  0.004079  [1144/1250 (92%)] Loss: 3.558599\n",
            "Train Epoch: 2 last_lr_used  0.004092  [1152/1250 (92%)] Loss: 3.675043\n",
            "Train Epoch: 2 last_lr_used  0.004104  [1160/1250 (93%)] Loss: 3.498731\n",
            "Train Epoch: 2 last_lr_used  0.004116  [1168/1250 (93%)] Loss: 3.316995\n",
            "Train Epoch: 2 last_lr_used  0.004128  [1176/1250 (94%)] Loss: 3.293769\n",
            "Train Epoch: 2 last_lr_used  0.004141  [1184/1250 (95%)] Loss: 3.582824\n",
            "Train Epoch: 2 last_lr_used  0.004153  [1192/1250 (95%)] Loss: 3.541717\n",
            "Train Epoch: 2 last_lr_used  0.004165  [1200/1250 (96%)] Loss: 3.509171\n",
            "Train Epoch: 2 last_lr_used  0.004178  [1208/1250 (97%)] Loss: 4.061545\n",
            "Train Epoch: 2 last_lr_used  0.004190  [1216/1250 (97%)] Loss: 4.291845\n",
            "Train Epoch: 2 last_lr_used  0.004202  [1224/1250 (98%)] Loss: 3.700428\n",
            "Train Epoch: 2 last_lr_used  0.004214  [1232/1250 (99%)] Loss: 3.497719\n",
            "Train Epoch: 2 last_lr_used  0.004227  [1240/1250 (99%)] Loss: 3.811851\n",
            "Train Epoch: 2 last_lr_used  0.004239  [1248/1250 (100%)] Loss: 3.578971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8yGeYCWnBCf2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}